{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far we have been modelling $E[y|\\mathbf{x}]$ through a multiple linear regression model, i.e., $E[y|\\mathbf{x}]=\\mathbf{x}^\\prime\\boldsymbol{\\beta}$. When $y\\in\\{0,1\\}$, the left-hand side of this equality becomes\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y|\\mathbf{x}]&=1\\times\\Pr\\{y=1|\\mathbf{x}\\}+0\\times(1-\\Pr\\{y=1|\\mathbf{x}\\})\\\\\n",
    "&=\\Pr\\{y=1|\\mathbf{x}\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "<ins>Linear Probability Model</ins>: If we stick to a multiple linear regression model one has\n",
    "\n",
    "$$\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}=\\mathbf{x}^\\prime\\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "‚ö†Ô∏è The $\\Pr\\{y=1|\\mathbf{x}\\}$ is not necessarily bounded between 0 and 1 as $\\mathbf{x}^\\prime\\boldsymbol{\\beta}$ moves from $-\\infty$ to $+\\infty$.\n",
    "\n",
    "<ins>Logistic Model</ins>: If we were to put the 'index' $\\mathbf{x}^\\prime\\boldsymbol{\\beta}$ inside the [standard logistic function](https://en.wikipedia.org/wiki/Logistic_function) $\\Lambda(u)=[1+\\exp{(-u)}]^{-1}$ and set\n",
    "\n",
    "$$\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}=\\Lambda(\\mathbf{x}^\\prime\\boldsymbol{\\beta}),\n",
    "$$\n",
    "\n",
    "this is known as the *Logistic Link Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "u <- seq(-5.10,5.10,length=100)\n",
    "plot(u,1/(1+exp(-u)),ylim=c(-0.10,1.10),\n",
    "     xlab=\"u\",ylab=\"\",type=\"l\",lty=1,col=\"black\",\n",
    "     las=1,lwd=2.1)\n",
    "lines(u,u,col=\"blue\",lwd=2.1)\n",
    "abline(h=0,lty=2,col=\"red\")\n",
    "abline(h=1,lty=2,col=\"red\")\n",
    "legend(-5.10, 0.95, legend=c(\"Logistic\", \"Linear\"),\n",
    "       col=c(\"black\", \"blue\"), lty=1, lwd=2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple manipulations shows that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}&=\\Lambda(\\mathbf{x}^\\prime\\boldsymbol{\\beta})=\\frac{1}{1+\\exp{(-\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}\\\\\n",
    "&=\\frac{\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}{1+\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}},\\\\\n",
    "1-\\Pr\\{y=1|\\mathbf{x}\\}&=\\frac{1}{1+\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}\\text{, and therefore}\\\\\n",
    "\\frac{\\Pr\\{y=1|\\mathbf{x}\\}}{1-\\Pr\\{y=1|\\mathbf{x}\\}}&=\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}.\\\\\n",
    "\\log{\\left(\\frac{\\Pr\\{y=1|\\mathbf{x}\\}}{1-\\Pr\\{y=1|\\mathbf{x}\\}}\\right)}&=\\mathbf{x}^\\prime\\boldsymbol{\\beta}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we define $\\Pr\\{y=1|\\mathbf{x}\\}=p(\\mathbf{x})$ as the probability of 'success,' then $p(\\mathbf{x})/[1-p(\\mathbf{x})]$ is called the [*odds*](https://en.wikipedia.org/wiki/Logit), and can take on *any* value between 0 and $\\infty$. Similarly the log of the odds is called the *log-odds* or *logit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p <- seq(0.01,0.99,length=100)\n",
    "plot(p,log(p/(1-p)),ylab=\"log(p/(1-p))\",\n",
    "     xlab=\"p\",type=\"l\",lty=1,col=\"black\",\n",
    "     las=1,lwd=2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>Example</ins>: If the probability of success is 0.8, i.e., $p(\\mathbf{x})=0.8$, then the odds equal $\\frac{0.8}{1-0.8}=4$ and one says that the odds of success is 4 to 1. Similarly if the probability of success is 0.5, i.e. $p(\\mathbf{x})=0.5$, then the odds equal $\\frac{0.5}{1-0.5}=1$ and we say that the odds of success is 1 to 1 in this case.\n",
    "\n",
    "**Note**: The parameter $\\beta_j$ represents how an increase of one unit of $x_j$ on average changes the *log-odds*, or equivalently $\\exp{(\\beta_j)}$ represents how an increase of one unit of $x_j$ on average changes the *odds*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª We are going to model female labor force participation decision based on various household characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "data(mroz)\n",
    "\n",
    "##  Obs:   753\n",
    "\n",
    "##  1. inlf                     =1 if in labor force, 1975\n",
    "##  2. hours                    hours worked, 1975\n",
    "##  3. kidslt6                  # kids < 6 years\n",
    "##  4. kidsge6                  # kids 6-18\n",
    "##  5. age                      woman's age in yrs\n",
    "##  6. educ                     years of schooling\n",
    "##  7. wage                     estimated wage from earns., hours\n",
    "##  8. repwage                  reported wage at interview in 1976\n",
    "##  9. hushrs                   hours worked by husband, 1975\n",
    "## 10. husage                   husband's age\n",
    "## 11. huseduc                  husband's years of schooling\n",
    "## 12. huswage                  husband's hourly wage, 1975\n",
    "## 13. faminc                   family income, 1975\n",
    "## 14. mtr                      fed. marginal tax rate facing woman\n",
    "## 15. motheduc                 mother's years of schooling\n",
    "## 16. fatheduc                 father's years of schooling\n",
    "## 17. unem                     unem. rate in county of resid.\n",
    "## 18. city                     =1 if live in SMSA (Standard Metropolitan Statistical Area)\n",
    "## 19. exper                    actual labor mkt exper\n",
    "## 20. nwifeinc                 (faminc - wage*hours)/1000\n",
    "## 21. lwage                    log(wage)\n",
    "## 22. expersq                  exper^2\n",
    "\n",
    "## specifying the outcome variable (y) and original predictors (X)\n",
    "outcome <- \"inlf\"\n",
    "predictors <- c(\"kidslt6\", \"kidsge6\", \"age\", \"educ\",\"exper\", \"hushrs\", \"husage\", \"huseduc\",\"huswage\", \n",
    "    \"nwifeinc\",\"mtr\",\"unem\",\"city\")\n",
    "\n",
    "## creating local copy with relevant variables\n",
    "data(\"mroz\", package = \"wooldridge\")\n",
    "mroz.copy <- subset(mroz, select = c(outcome, predictors))\n",
    "\n",
    "head(mroz.copy,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "üíª The following set of commands will *add* **all** possible demeaned cross-products among features excluding ```city``` to the ```mroz.copy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'tidyverse' package if not previously installed\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "\n",
    "## installing the 'caret' package if not previously installed\n",
    "if (!require(caret)) install.packages('caret')\n",
    "\n",
    "## installing the 'MLmetrics' package if not previously installed\n",
    "if (!require(MLmetrics)) install.packages(\"MLmetrics\")\n",
    "library(MLmetrics)\n",
    "\n",
    "## demeaning all predictors, renaming them and saving their variable names\n",
    "x <- scale(model.matrix(inlf ~ .-city, mroz.copy)[, -1], center = TRUE, scale = FALSE)\n",
    "colnames(x) <- paste(\"d_\", colnames(x), sep = \"\")\n",
    "drops <- colnames(x)\n",
    "\n",
    "## attaching the demeaned predictors to a copy of the original data set\n",
    "mroz.copy <- cbind(mroz.copy, x)\n",
    "\n",
    "d.predictors <- paste('(',paste(drops, collapse = \"+\"),')^2',sep=\"\")\n",
    "xx <- model.matrix(lm(as.formula(paste0(outcome, \"~\", d.predictors)),data=mroz.copy))[,-1]\n",
    "\n",
    "mroz.copy <- cbind(mroz.copy[, !(colnames(mroz.copy) %in% drops)],xx[,!(colnames(xx) %in% drops)])\n",
    "colnames(mroz.copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "üíª We now partition the augmented data set into the ```train.data``` (75% of the original observations) and ```test.data``` (25% of the original observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## split the data into training and test set\n",
    "set.seed(2020)\n",
    "training.samples <- mroz.copy$inlf %>% \n",
    "  createDataPartition(p = 0.75, list = FALSE)\n",
    "train.data  <- mroz.copy[training.samples, ]\n",
    "test.data <- mroz.copy[-training.samples, ]\n",
    "\n",
    "## printing the response variable for the training set\n",
    "set.seed(2020)\n",
    "sample(train.data$inlf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Firstly remember that if we have a *random sample*, then the joint probability of observing the sequence of $n$ ones and zeroes above is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&(1-p(\\mathbf{x}_1))\\cdot p(\\mathbf{x}_2)\\cdot (1-p(\\mathbf{x}_3)\\cdot (1-p(\\mathbf{x}_4))\\cdot p(\\mathbf{x}_5)\\ldots\\\\\n",
    "& = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i}[1-p(\\mathbf{x}_i)]^{1-y_i}\\\\\n",
    "& = \\prod_{i=1}^{n} \\Lambda(\\mathbf{x}_i^\\prime\\boldsymbol{\\beta})^{y_i}[1-\\Lambda(\\mathbf{x}_i^\\prime\\boldsymbol{\\beta})]^{1-y_i}\\\\\n",
    "&=L(\\boldsymbol{\\beta}|y_1,\\ldots,y_n;\\mathbf{x}_1,\\ldots,\\mathbf{x}_n)\\\\\n",
    "&=:L_n(\\boldsymbol{\\beta}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The function $L_n(\\boldsymbol{\\beta})$ is called the [*likelihood function*](https://en.wikipedia.org/wiki/Likelihood_function) and if we take the natural logarithm, i.e.,\n",
    "\n",
    "$$\\ell_n(\\boldsymbol{\\beta})=\\log{L_n(\\boldsymbol{\\beta})}$$\n",
    "\n",
    "is called the *log-likelihood function*. Therefore, we can estimate $\\boldsymbol{\\beta}$ by maximizing the *log-likelihood function* as this equivalent as to finding the $\\boldsymbol{\\beta}$ that maximizes the joint probability to observe the sample we have, i.e., $\\widehat{\\boldsymbol{\\beta}}=\\underset{\\mathbf{b}}{\\text{arg max }} \\ell_n(\\mathbf{b})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "üíª We start by estimating the base model, i.e, $\\mathbf{x}$=(1,```kidslt6```,```kidsge6```,```age```,```educ```,```exper```,```hushrs```,```husage```,```huseduc```,```huswage```,```nwifeinc```,```mtr```,```unem```,```city```)$^\\prime$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## fit the model\n",
    "b.predictors <- paste(predictors,collapse = \"+\")\n",
    "model <- glm( as.formula(paste0(outcome, \"~\", b.predictors)), data = train.data, family = binomial)\n",
    "\n",
    "## printing the exp() of the model estimated coefficients\n",
    "exp(coef(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Interpreting the Logit Coefficients](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª Holding everything else constant, one extra year of ```educ``` will make the odds of a woman to be working for wages to be 28.799% higher. The odds of a woman working for wages in a ```city``` is 1.002 times that if she were to live in a rural area (non-SMSA) instead. Having 1 extra ```kidslt6``` will reduce the odds of a woman to be working for wages by a factor of 0.342."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicted Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª Once we have obtained $\\widehat{\\boldsymbol{\\beta}}$ from the ```train.data``` set we can plug-in the features in the ```test.data``` set and calculate the predicted probabilities to be in the labor force for observations in the validation set. If the predicted probability for observation $j$ in the validaton set is above 0.5 we can predict a 'success,' i.e., $\\widehat{y}_j=1$ and $\\widehat{y}_j=0$ otherwise. Since we *do* observe the actual outcome for said observation, i.e., $y_j$ we can count how many times our model predicted the correct outcome, and apply [McNemar's test](https://en.wikipedia.org/wiki/McNemar%27s_test) to the resulting contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## making predictions\n",
    "probabilities <- model %>% predict(test.data, type = \"response\")\n",
    "predicted.classes <- ifelse(probabilities > 0.5, 1, 0)\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = factor(test.data$inlf,labels=c(\"no\",\"yes\")),\n",
    "                reference = factor(predicted.classes,labels=c(\"no\",\"yes\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theme_set(theme_bw())\n",
    "\n",
    "xb <- predict(model, type = \"link\")\n",
    "\n",
    "data.frame(train.data,xb=xb) %>%\n",
    "      ggplot(aes(xb, inlf)) +\n",
    "      geom_point(alpha = 0.2) +\n",
    "      geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n",
    "      labs(\n",
    "        title = \"Logistic Regression Model\", \n",
    "        x = bquote(x  * hat(beta)),\n",
    "        y = \"Probability of being in-labor-force\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Elastic Net_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª Now assume that we are interested in fitting a model with a larger set of features, like the one that contains the initial features in the base model *plus* all possible cross-products of the demeaned features excluding ```city```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for high-dimensional estimation\n",
    "if (!require(glmnet)) install.packages(\"glmnet\")\n",
    "library(glmnet)\n",
    "\n",
    "## creating the feature matrix & outcome for the train data\n",
    "x.train <- as.matrix(train.data[,-1])\n",
    "y.train <- train.data$inlf\n",
    "\n",
    "## creating the response variable for the train data\n",
    "x.test <- as.matrix(test.data[,-1])\n",
    "y.test <- test.data$inlf\n",
    "\n",
    "colnames(train.data[,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case we can implement the Elastic Net regression as explained before, i.e.,\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}=\\underset{\\mathbf{b}}{\\text{arg min }} -\\ell_n(\\mathbf{b}) +\\lambda\\left((1-\\alpha)\\|\\mathbf{b}\\|_{2}^{2}+\\alpha\\|\\mathbf{b}\\|_{1}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(24)\n",
    "cv.lasso <- cv.glmnet(x.train, y.train,, nfolds = 10, alpha = 1, family = \"binomial\")\n",
    "plot(cv.lasso)\n",
    "\n",
    "cv.lasso$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "coef(cv.lasso, cv.lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# final model with optimal lambda\n",
    "lasso.model <- glmnet(x.train, y.train, alpha = 1, family = \"binomial\",\n",
    "                      lambda = cv.lasso$lambda.min)\n",
    "probabilities <- lasso.model %>% predict(newx = x.test)\n",
    "predicted.classes <- ifelse(probabilities > 0.5, \"1\", \"0\")\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = factor(test.data$inlf,labels=c(\"no\",\"yes\")),\n",
    "                reference = factor(predicted.classes,labels=c(\"no\",\"yes\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## setting up a grid range of lambda values\n",
    "#lambda <- seq(0.009,0.011,length=10)\n",
    "\n",
    "## LASSO: building the model using 10-fold CV repeated 5 times\n",
    "#set.seed(2020)\n",
    "#lasso <- train(inlf ~ ., data = data.frame(inlf = factor(train.data$inlf,labels=c(\"no\",\"yes\")), train.data[,-1]), method = \"glmnet\", \n",
    "#    trControl = trainControl(\"repeatedcv\",number = 10,repeats=5,verboseIter = F,summaryFunction = twoClassSummary,classProbs = T), tuneGrid = expand.grid(alpha = 1, \n",
    "#        lambda = lambda),family=\"binomial\",metric=\"ROC\")\n",
    "\n",
    "\n",
    "## LASSO: model coefficients\n",
    "#coef(lasso$finalModel, lasso$bestTune$lambda)\n",
    "\n",
    "## LASSO: making predictions\n",
    "#probabilities <- lasso %>% predict(data.frame(lprice = factor(test.data$inlf,labels=c(\"no\",\"yes\")), test.data[,-1]))\n",
    "\n",
    "# Model accuracy rate\n",
    "#confusionMatrix(data = factor(test.data$inlf,labels=c(\"no\",\"yes\")),\n",
    "#                reference = factor(probabilities,labels=c(\"no\",\"yes\")))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
