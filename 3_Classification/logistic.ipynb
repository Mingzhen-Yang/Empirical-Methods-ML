{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far we have been modelling $E[y|\\mathbf{x}]$ through a multiple linear regression model, i.e., $E[y|\\mathbf{x}]=\\mathbf{x}^\\prime\\boldsymbol{\\beta}$. When $y\\in\\{0,1\\}$, the left-hand side of this equality becomes\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[y|\\mathbf{x}]&=1\\times\\Pr\\{y=1|\\mathbf{x}\\}+0\\times(1-\\Pr\\{y=1|\\mathbf{x}\\})\\\\\n",
    "&=\\Pr\\{y=1|\\mathbf{x}\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "<ins>Linear Probability Model</ins>: If we stick to a multiple linear regression model one has\n",
    "\n",
    "$$\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}=\\mathbf{x}^\\prime\\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "⚠️ The $\\Pr\\{y=1|\\mathbf{x}\\}$ is not necessarily bounded between 0 and 1 as $\\mathbf{x}^\\prime\\boldsymbol{\\beta}$ moves from $-\\infty$ to $+\\infty$.\n",
    "\n",
    "<ins>Logistic Model</ins>: If we were to put the 'index' $\\mathbf{x}^\\prime\\boldsymbol{\\beta}$ inside the [standard logistic function](https://en.wikipedia.org/wiki/Logistic_function) $\\Lambda(u)=[1+\\exp{(-u)}]^{-1}$ and set\n",
    "\n",
    "$$\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}=\\Lambda(\\mathbf{x}^\\prime\\boldsymbol{\\beta}),\n",
    "$$\n",
    "\n",
    "this is known as the *Logistic Link Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAAP9NTU1oaGh8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\nAUICBAgJECAkQOB/ycz6kO2tVm4AAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u <- seq(-5.10,5.10,length=100)\n",
    "plot(u,1/(1+exp(-u)),ylim=c(-0.10,1.10),\n",
    "     xlab=\"u\",ylab=\"\",type=\"l\",lty=1,col=\"black\",\n",
    "     las=1,lwd=2.1)\n",
    "lines(u,u,col=\"blue\",lwd=2.1)\n",
    "abline(h=0,lty=2,col=\"red\")\n",
    "abline(h=1,lty=2,col=\"red\")\n",
    "legend(-5.10, 0.95, legend=c(\"Logistic\", \"Linear\"),\n",
    "       col=c(\"black\", \"blue\"), lty=1, lwd=2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple manipulations shows that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Pr\\{y=1|\\mathbf{x}\\}&=\\Lambda(\\mathbf{x}^\\prime\\boldsymbol{\\beta})=\\frac{1}{1+\\exp{(-\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}\\\\\n",
    "&=\\frac{\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}{1+\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}},\\\\\n",
    "1-\\Pr\\{y=1|\\mathbf{x}\\}&=\\frac{1}{1+\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}}\\text{, and therefore}\\\\\n",
    "\\frac{\\Pr\\{y=1|\\mathbf{x}\\}}{1-\\Pr\\{y=1|\\mathbf{x}\\}}&=\\exp{(\\mathbf{x}^\\prime\\boldsymbol{\\beta})}.\\\\\n",
    "\\log{\\left(\\frac{\\Pr\\{y=1|\\mathbf{x}\\}}{1-\\Pr\\{y=1|\\mathbf{x}\\}}\\right)}&=\\mathbf{x}^\\prime\\boldsymbol{\\beta}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we define $\\Pr\\{y=1|\\mathbf{x}\\}=p(\\mathbf{x})$ as the probability of 'success,' then $p(\\mathbf{x})/[1-p(\\mathbf{x})]$ is called the [*odds*](https://en.wikipedia.org/wiki/Logit), and can take on *any* value between 0 and $\\infty$. Similarly the log of the odds is called the *log-odds* or *logit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nBAgJAoQEAf8DIcAVIjS1H3MAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p <- seq(0.01,0.99,length=100)\n",
    "plot(p,log(p/(1-p)),ylab=\"log(p/(1-p))\",\n",
    "     xlab=\"p\",type=\"l\",lty=1,col=\"black\",\n",
    "     las=1,lwd=2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>Example</ins>: If the probability of success is 0.8, i.e., $p(\\mathbf{x})=0.8$, then the odds equal $\\frac{0.8}{1-0.8}=4$ and one says that the odds of success is 4 to 1. Similarly if the probability of success is 0.5, i.e. $p(\\mathbf{x})=0.5$, then the odds equal $\\frac{0.5}{1-0.5}=1$ and we say that the odds of success is 1 to 1 in this case.\n",
    "\n",
    "**Note**: The parameter $\\beta_j$ represents how an increase of one unit of $x_j$ on average changes the *log-odds*, or equivalently $\\exp{(\\beta_j)}$ represents how an incrase of one unit of $x_j$ on average changes the *odds*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 We are going to model female labor force participation decision based on various household characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "data(mroz)\n",
    "\n",
    "##  Obs:   753\n",
    "\n",
    "##  1. inlf                     =1 if in labor force, 1975\n",
    "##  2. hours                    hours worked, 1975\n",
    "##  3. kidslt6                  # kids < 6 years\n",
    "##  4. kidsge6                  # kids 6-18\n",
    "##  5. age                      woman's age in yrs\n",
    "##  6. educ                     years of schooling\n",
    "##  7. wage                     estimated wage from earns., hours\n",
    "##  8. repwage                  reported wage at interview in 1976\n",
    "##  9. hushrs                   hours worked by husband, 1975\n",
    "## 10. husage                   husband's age\n",
    "## 11. huseduc                  husband's years of schooling\n",
    "## 12. huswage                  husband's hourly wage, 1975\n",
    "## 13. faminc                   family income, 1975\n",
    "## 14. mtr                      fed. marginal tax rate facing woman\n",
    "## 15. motheduc                 mother's years of schooling\n",
    "## 16. fatheduc                 father's years of schooling\n",
    "## 17. unem                     unem. rate in county of resid.\n",
    "## 18. city                     =1 if live in SMSA (Standard Metropolitan Statistical Area)\n",
    "## 19. exper                    actual labor mkt exper\n",
    "## 20. nwifeinc                 (faminc - wage*hours)/1000\n",
    "## 21. lwage                    log(wage)\n",
    "## 22. expersq                  exper^2\n",
    "\n",
    "## specifying the outcome variable (y) and original predictors (X)\n",
    "outcome <- \"inlf\"\n",
    "predictors <- c(\"kidslt6\", \"kidsge6\", \"age\", \"educ\",\"exper\", \"hushrs\", \"husage\", \"huseduc\",\"huswage\", \n",
    "    \"nwifeinc\",\"mtr\",\"unem\",\"city\")\n",
    "\n",
    "## creating local copy with relevant variables\n",
    "data(\"mroz\", package = \"wooldridge\")\n",
    "mroz.copy <- subset(mroz, select = c(outcome, predictors))\n",
    "\n",
    "head(mroz.copy,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "💻 The following set of commands will *add* **all** possible demeaned cross-products among features excluding ```city``` to the ```mroz.copy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'tidyverse' package if not previously installed\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "\n",
    "## installing the 'caret' package if not previously installed\n",
    "if (!require(caret)) install.packages('caret')\n",
    "\n",
    "## installing the 'MLmetrics' package if not previously installed\n",
    "if (!require(MLmetrics)) install.packages(\"MLmetrics\")\n",
    "library(MLmetrics)\n",
    "\n",
    "## demeaning all predictors, renaming them and saving their variable names\n",
    "x <- scale(model.matrix(inlf ~ .-city, mroz.copy)[, -1], center = TRUE, scale = FALSE)\n",
    "colnames(x) <- paste(\"d_\", colnames(x), sep = \"\")\n",
    "drops <- colnames(x)\n",
    "\n",
    "## attaching the demeaned predictors to a copy of the original data set\n",
    "mroz.copy <- cbind(mroz.copy, x)\n",
    "\n",
    "d.predictors <- paste('(',paste(drops, collapse = \"+\"),')^2',sep=\"\")\n",
    "xx <- model.matrix(lm(as.formula(paste0(outcome, \"~\", d.predictors)),data=mroz.copy))[,-1]\n",
    "\n",
    "mroz.copy <- cbind(mroz.copy[, !(colnames(mroz.copy) %in% drops)],xx[,!(colnames(xx) %in% drops)])\n",
    "colnames(mroz.copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "💻 We now partition the augmented data set into the ```train.data``` (75% of the original observations) and ```test.data``` (25% of the original observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## split the data into training and test set\n",
    "set.seed(2020)\n",
    "training.samples <- mroz.copy$inlf %>% \n",
    "  createDataPartition(p = 0.75, list = FALSE)\n",
    "train.data  <- mroz.copy[training.samples, ]\n",
    "test.data <- mroz.copy[-training.samples, ]\n",
    "\n",
    "## printing the response variable for the training set\n",
    "set.seed(2020)\n",
    "sample(train.data$inlf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Firstly remember that if we have a *random sample*, then the joint probability of observing the sequence of $n$ ones and zeroes above is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&(1-p(\\mathbf{x}_1))\\cdot p(\\mathbf{x}_2)\\cdot (1-p(\\mathbf{x}_3)\\cdot p(\\mathbf{x}_4)\\cdot p(\\mathbf{x}_5)\\ldots\\\\\n",
    "& = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i}[1-p(\\mathbf{x}_i)]^{1-y_i}\\\\\n",
    "& = \\prod_{i=1}^{n} \\Lambda(\\mathbf{x}_i^\\prime\\boldsymbol{\\beta})^{y_i}[1-\\Lambda(\\mathbf{x}_i^\\prime\\boldsymbol{\\beta})]^{1-y_i}\\\\\n",
    "&=L(\\boldsymbol{\\beta}|y_1,\\ldots,y_n;\\mathbf{x}_1,\\ldots,\\mathbf{x}_n)\\\\\n",
    "&=:L_n(\\boldsymbol{\\beta}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The function $L_n(\\boldsymbol{\\beta})$ is called the [*likelihood function*](https://en.wikipedia.org/wiki/Likelihood_function) and if we take the natural logarithm, i.e.,\n",
    "\n",
    "$$\\ell_n(\\boldsymbol{\\beta})=\\log{L_n(\\boldsymbol{\\beta})}$$\n",
    "\n",
    "is called the *log-likelihood function*. Therefore, we can estimate $\\boldsymbol{\\beta}$ by maximizing the *log-likelihood function* as this equivalent as to finding the $\\boldsymbol{\\beta}$ that maximizes the joint probability to observe the sample we have, i.e., $\\widehat{\\boldsymbol{\\beta}}=\\underset{\\mathbf{b}}{\\text{arg max }} \\ell_n(\\mathbf{b})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "💻 We start by estimating the base model, i.e, $\\mathbf{x}$=(1,```kidslt6```,```kidsge6```,```age```,```educ```,```exper```,```hushrs```,```husage```,```huseduc```,```huswage```,```nwifeinc```,```mtr```,```unem```,```city```)$^\\prime$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## fit the model\n",
    "b.predictors <- paste(predictors,collapse = \"+\")\n",
    "model <- glm( as.formula(paste0(outcome, \"~\", b.predictors)), data = train.data, family = binomial)\n",
    "\n",
    "## printing the exp() of the model estimated coefficients\n",
    "exp(coef(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Interpreting the Logit Coefficients](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 Holding everything else constant, one extra year of ```educ``` will make the odds of a woman to be working for wages to be 28.324% higher. The odds of a woman working for wages in a ```city``` is 1.029 times that if she were to live in a rural area (non-SMSA) instead. Having 1 extra ```kidslt6``` will reduce the odds of a woman to be working for wages by a factor of 0.266."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicted Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 Once we have obtained $\\widehat{\\boldsymbol{\\beta}}$ from the ```train.data``` set we can plug-in the features in the ```test.data``` set and calculate the predicted probabilities to be in the labor force for observations in the validation set. If the predicted probability for observation $j$ in the validaton set is above 0.5 we can predict a 'success,' i.e., $\\widehat{y}_j=1$ and $\\widehat{y}_j=0$ otherwise. Since we *do* observe the actual outcome for said observation, i.e., $y_j$ we can count how many times our model predicted the correct outcome, and apply [McNemar's test](https://en.wikipedia.org/wiki/McNemar%27s_test) to the resulting contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## making predictions\n",
    "probabilities <- model %>% predict(test.data, type = \"response\")\n",
    "predicted.classes <- ifelse(probabilities > 0.5, 1, 0)\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = factor(test.data$inlf,labels=c(\"no\",\"yes\")),\n",
    "                reference = factor(predicted.classes,labels=c(\"no\",\"yes\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theme_set(theme_bw())\n",
    "\n",
    "xb <- predict(model, type = \"link\")\n",
    "\n",
    "data.frame(train.data,xb=xb) %>%\n",
    "      ggplot(aes(xb, inlf)) +\n",
    "      geom_point(alpha = 0.2) +\n",
    "      geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n",
    "      labs(\n",
    "        title = \"Logistic Regression Model\", \n",
    "        x = bquote(x  * hat(beta)),\n",
    "        y = \"Probability of being in-labor-force\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Elastic Net_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 Now assume that we are interested in fitting a model with a larger set of features, like the one that contains the initial features in the base model *plus* all possible cross-products of the demeaned features excluding ```city```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for high-dimensional estimation\n",
    "if (!require(glmnet)) install.packages(\"glmnet\")\n",
    "library(glmnet)\n",
    "\n",
    "## creating the feature matrix & outcome for the train data\n",
    "x.train <- as.matrix(train.data[,-1])\n",
    "y.train <- train.data$inlf\n",
    "\n",
    "## creating the response variable for the train data\n",
    "x.test <- as.matrix(test.data[,-1])\n",
    "y.test <- test.data$inlf\n",
    "\n",
    "colnames(train.data[,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case we can implement the Elastic Net regression as explained before, i.e.,\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}=\\underset{\\mathbf{b}}{\\text{arg min }} -\\ell_n(\\mathbf{b}) +\\lambda\\left((1-\\alpha)\\|\\mathbf{b}\\|_{2}^{2}+\\alpha\\|\\mathbf{b}\\|_{1}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(24)\n",
    "cv.lasso <- cv.glmnet(x.train, y.train,, nfolds = 10, alpha = 1, family = \"binomial\")\n",
    "plot(cv.lasso)\n",
    "\n",
    "cv.lasso$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "coef(cv.lasso, cv.lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# final model with optimal lambda\n",
    "lasso.model <- glmnet(x.train, y.train, alpha = 1, family = \"binomial\",\n",
    "                      lambda = cv.lasso$lambda.min)\n",
    "probabilities <- lasso.model %>% predict(newx = x.test)\n",
    "predicted.classes <- ifelse(probabilities > 0.5, \"1\", \"0\")\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = factor(test.data$inlf,labels=c(\"no\",\"yes\")),\n",
    "                reference = factor(predicted.classes,labels=c(\"no\",\"yes\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## setting up a grid range of lambda values\n",
    "#lambda <- seq(0.009,0.011,length=10)\n",
    "\n",
    "## LASSO: building the model using 10-fold CV repeated 5 times\n",
    "#set.seed(2020)\n",
    "#lasso <- train(inlf ~ ., data = data.frame(inlf = factor(train.data$inlf,labels=c(\"no\",\"yes\")), train.data[,-1]), method = \"glmnet\", \n",
    "#    trControl = trainControl(\"repeatedcv\",number = 10,repeats=5,verboseIter = F,summaryFunction = twoClassSummary,classProbs = T), tuneGrid = expand.grid(alpha = 1, \n",
    "#        lambda = lambda),family=\"binomial\",metric=\"ROC\")\n",
    "\n",
    "\n",
    "## LASSO: model coefficients\n",
    "#coef(lasso$finalModel, lasso$bestTune$lambda)\n",
    "\n",
    "## LASSO: making predictions\n",
    "#probabilities <- lasso %>% predict(data.frame(lprice = factor(test.data$inlf,labels=c(\"no\",\"yes\")), test.data[,-1]))\n",
    "\n",
    "# Model accuracy rate\n",
    "#confusionMatrix(data = factor(test.data$inlf,labels=c(\"no\",\"yes\")),\n",
    "#                reference = factor(probabilities,labels=c(\"no\",\"yes\")))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
