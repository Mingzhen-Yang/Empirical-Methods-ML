{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# _Multiple_ Linear Regression (MLR) Model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The Model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the population we have <ins>scalar</ins> random variables, $[Y,X_1,\\ldots,X_{k},e]$, that fulfill the following relationship\n\n$$\n\\begin{align*}\nY=\\beta_0+\\beta_1 X_1+\\ldots+\\beta_k X_k+e\\text{,}\n\\end{align*}\n$$\n\nwhere $E[e|X_1,\\ldots,X_{k}]=0$, there are no exact linear relationships among the set of regressors (covariates, confounders, independent variables, predictors, etc.) $[X_1,\\ldots,X_{k}]$, and var$(e|X_1,\\ldots,X_{k})<+\\infty$. The scalar random variable, $Y$, is called the outcome, or the dependent variable.\n\n\n\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Conditional_expectation\" style=\"color: #cc0000\">Conditional Expectation</a></p>\n\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Conditional_variance\" style=\"color: #cc0000\">Conditional Variance</a></p>\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The Data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We observe a _random sample_ of $n$ observations taken from $[Y,X_1,\\ldots,X_{k},e]$, i.e., $\\{(y_i,x_{i,1},\\ldots,x_{i,k}):i=1,\\ldots,n\\}$. Therefore one has\n\n$$\n\\begin{array}\n[c]{c}\ny_{1}=\\beta_{0}+\\beta_{1}x_{1,1}+\\cdots+\\beta_{k}x_{1,k}+e_{1}\\\\\ny_{2}=\\beta_{0}+\\beta_{1}x_{2,1}+\\cdots+\\beta_{k}x_{2,k}+e_{2}\\\\\ny_{3}=\\beta_{0}+\\beta_{1}x_{3,1}+\\cdots+\\beta_{k}x_{3,k}+e_{3}\\\\\n\\vdots\\\\\ny_{n}=\\beta_{0}+\\beta_{1}x_{n,1}+\\cdots+\\beta_{k}x_{n,k}+e_{n}\n\\end{array}\n$$ or equivalently\n$$\\left[\n\\begin{array}\n[c]{c}\ny_{1}\\\\\ny_{2}\\\\\ny_{3}\\\\\n\\vdots\\\\\ny_{n}\n\\end{array}\n\\right]  =\\left[\n\\begin{array}\n[c]{cccc}\n1 & x_{1,1} & \\cdots & x_{1,k}\\\\\n1 & x_{2,1} & \\cdots & x_{2,k}\\\\\n1 & x_{3,1} & \\cdots & x_{3,k}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{n,1} & \\cdots & x_{n,k}\n\\end{array}\n\\right]  \\left[\n\\begin{array}\n[c]{c}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k}\n\\end{array}\n\\right]  +\\left[\n\\begin{array}\n[c]{c}\ne_{1}\\\\\ne_{2}\\\\\ne_{3}\\\\\n\\vdots\\\\\ne_{n}\n\\end{array}\n\\right]\n$$ that can be rewritten in **matrix form**  as\n\n$$\n\\begin{align*}\n\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e}\\text{,}\n\\end{align*}\n$$\n\nwhere $\\mathbf{y}$ is a $n\\times 1$ [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)), $\\mathbf{X}$ is a $n\\times(k+1)$ [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)) - sometimes called the [*design matrix*](https://en.wikipedia.org/wiki/Design_matrix), $\\mathbf{\\beta}$ is a $(k+1)\\times 1$ vector of <ins>unknown</ins> parameters, and $\\mathbf{e}$ is a $n\\times 1$ vector."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## removing everything from memory\nrm(list=ls())\n## turning all warnings off\noptions(warn=-1)\n\n## installing the 'wooldridge' package if not previously installed\nif (!require(wooldridge)) install.packages('wooldridge')\n\n## loading the packages\nlibrary(wooldridge)\n\ndata(hprice2)\n\n##  hprice2\n##  Obs:   506\n\n##  1. price                    median housing price, $\n##  2. crime                    crimes committed per capita\n##  3. nox                      nitrous oxide, parts per 100 mill.\n##  4. rooms                    avg number of rooms per house\n##  5. dist                     weighted dist. to 5 employ centers\n##  6. radial                   accessibiliy index to radial hghwys\n##  7. proptax                  property tax per $1000\n##  8. stratio                  average student-teacher ratio\n##  9. lowstat                  % of people 'lower status'\n## 10. lprice                   log(price)\n## 11. lnox                     log(nox)\n## 12. lproptax                 log(proptax)\n\nhead(hprice2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The Assumptions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**<span style=\"color:blue\">Assumption MLR.1:</span>** $\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e}$."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<ins>Example</ins>: We specify the following model \n\n$$\n\\texttt{lprice}=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+e\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "💻 We can use ```as.formula``` to specify the model."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## specifying the outcome variable (y) and regressors (X)\noutcome <- \"lprice\"\npredictors <- c(\"lnox\", \"lproptax\", \"crime\", \"rooms\", \"dist\", \"radial\", \"stratio\", \"lowstat\")\n\n## creating a specification of the linear model\nf <- as.formula(\n                paste(outcome, \n                      paste(predictors, collapse = \" + \"), \n                      sep = \" ~ \")\n                )\nprint(f)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**<span style=\"color:blue\">Assumption MLR.2:</span>** $\\{(y_i,x_{i,1},\\ldots,x_{i,k}):i=1,\\ldots,n\\}$ is a random sample (independent and identically distributed - i.i.d.)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "💻 ```head``` allows us to visualize the structure of the data."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "head(subset(hprice2,select=c(outcome,predictors)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**<span style=\"color:blue\">Assumption MLR.3:</span>** rank$[E(\\mathbf{x}_i\\mathbf{x}_i^\\prime)]=k+1$, where $\\mathbf{x}_i^\\prime=[1,x_{i,1},\\ldots,x_{i,k}]$. <p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Rank_(linear_algebra)\" style=\"color: #cc0000\">Rank of a Matrix</a></p>\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Transpose\" style=\"color: #cc0000\">Transpose</a></p> <p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Expected_value\" style=\"color: #cc0000\">Expected Value</a></p>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "💻 ```model.matrix``` creates a design matrix based on the declared predictors and includes a vector of ones by default as the first column."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## asking R to print the design matrix for the chosen model\nX <- model.matrix(f,data=hprice2)\ndim(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## calculating & printing the sample counterpart of E[xx']\nX.X.n <- t(X)%*%X/nrow(X)\nX.X.n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## asking R to calculate the actual rank of the estimated E[xx']\nqr(X.X.n)$rank",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**<span style=\"color:blue\">Assumption MLR.4:</span>** $E[\\mathbf{e}|\\mathbf{X}]=\\mathbf{0}$.\n\n$$\nE[\\mathbf{e}|\\mathbf{X}]=\\left[\n\\begin{array}\n[c]{c}\nE[e_{1}|\\mathbf{X}]\\\\\nE[e_{2}|\\mathbf{X}]\\\\\nE[e_{3}|\\mathbf{X}]\\\\\n\\vdots\\\\\nE[e_{n}|\\mathbf{X}]\n\\end{array}\n\\right]  =\\left[\n\\begin{array}\n[c]{c}\n0\\\\\n0\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\n\\right]=\\mathbf{0}.  $$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**<span style=\"color:blue\">Assumption MLR.5:</span>** var$(\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X})=E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\\mathbf{D}$, i.e.,\n\n$$\nE[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\n\\begin{bmatrix}\nE[e_{1}^{2}|\\mathbf{X}] & 0 & \\cdots & 0\\\\\n0 & E[e_{2}^{2}|\\mathbf{X}] & \\cdots & 0\\\\\n0 & 0 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\cdots & E[e_{n}^{2}|\\mathbf{X}]\n\\end{bmatrix}  = \\mathbf{D}\n$$\n\nwhere $\\vert\\mathbf{D}\\vert < +\\infty$.\n\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Determinant\" style=\"color: #cc0000\">Determinant</a></p>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The *Ordinary Least Squares* (OLS) Estimator"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Define the Sum of Squared Errors ($\\mathrm{SSE}$) as a function of any candidate guess, $\\mathbf{b}=[b_{0},b_{1},\\cdots,b_{k}]^{\\prime}$, for the unknown $[\\beta_{0},\\beta_{1},\\beta_{2},\\cdots,\\beta_{k}]^{\\prime}\\equiv\\mathbf{\\beta}$, i.e.,\n\n$$\n\\mathrm{SSE}(\\mathbf{b})=\\Sigma_{i=1}^{n}(y_{1}-b_{0}-b_{1}x_{1}-b_{2}x_{2}\n-\\cdots-b_{k}x_{k})^{2}=(\\mathbf{y}-\\mathbf{Xb})^{\\prime}(\\mathbf{y}\n-\\mathbf{Xb})\n$$\n\nBy standard [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus) one has\n$$\n\\frac{\\partial \\mathrm{SSE}(\\mathbf{b})}{\\partial\\mathbf{b}}=\\left[\n\\begin{array}\n[c]{c}\n\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{0}\\\\\n\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}\\\\\n\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\\\\n\\vdots\\\\\n\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\n\\end{array}\n\\right]  =-2\\mathbf{X}^{\\prime}(\\mathbf{y}-\\mathbf{Xb})\\text{,}\n$$\n\nand\n\n$$\n\\frac{\\partial^{2}\\mathrm{SSE}(\\mathbf{b})}{\\partial\\mathbf{b}\\partial\\mathbf{b}\n^{\\prime}}=\\left[\n\\begin{array}\n[c]{cccc}\n\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{0}^{2} & \\partial^{2}\\mathrm{SSE}(\\mathbf{b}\n)/\\partial b_{0}\\partial b_{1} & \\cdots & \\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial\nb_{0}\\partial b_{k}\\\\\n\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}\\partial b_{0} & \\partial\n^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}^{2} & \\cdots & \\partial^{2}\\mathrm{SSE}(\\mathbf{b}\n)/\\partial b_{1}\\partial b_{k}\\\\\n\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{0} & \\partial\n^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{1} & \\cdots & \\partial\n^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{k}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\\partial b_{0} & \\partial\n^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\\partial b_{1} & \\cdots & \\partial\n^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}^{2}\n\\end{array}\n\\right]  =2\\mathbf{X}^{\\prime}\\mathbf{X}\\text{,}\n$$\n\nwhere $\\mathbf{X}^{\\prime}\\mathbf{X}$ is a [positive definite matrix](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) by **<span style=\"color:blue\">Assumption MLR.3</span>**."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## printing 2X'X\n2*t(X)%*%X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Therefore $\\left.  \\partial \\mathrm{SSE}(\\mathbf{b})/\\partial\\mathbf{b}\\right\\vert _{\\mathbf{b}=\\widehat{\\boldsymbol{\\beta}}\n}=\\mathbf{0}$ defines a maxima, i.e.,\n\n$$\\widehat{\\boldsymbol{\\beta}}=(\\mathbf{X}\n^{\\prime}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}.$$\n\nHere we have used the\nnotation $\\mathbf{A}^{-1}$ to denote the inverse of a matrix $\\mathbf{A}$.\n\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" style=\"color: #cc0000\">Euclidean Distance</a></p>\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Invertible_matrix\" style=\"color: #cc0000\">Inverse</a></p>\n<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Maxima_and_minima\" style=\"color: #cc0000\">Maxima</a></p>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "💻 Calculating the OLS estimator by hand first and then using the highly optimize ```lm``` command."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## calculating OLS by hand\nsolve(t(X)%*%X)%*%(t(X)%*%hprice2$lprice)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## calculating OLS using the `lm' command in R\ncoef(lm(f,data=hprice2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The _Algebra_ of the OLS Estimator"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "💻 We now will save the ```lm``` object and called it ```ols```"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ols <- lm(f,data=hprice2)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Fitted Values*: $\\widehat{\\mathbf{y}}=\\mathbf{X}\\widehat{\\boldsymbol{\\beta}}$.\n\n*Residuals*: $\\widehat{\\mathbf{e}}=\\mathbf{y}-\\mathbf{X} \\widehat{\\boldsymbol{\\beta}}$."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## OLS: printing the first 6 outcome, fitted values, & residuals\nhead(data.frame(y=hprice2$lprice,y.hat=fitted(ols),e.hat=resid(ols)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Orthogonality*: $\\mathbf{X}^{\\prime}\\widehat{\\mathbf{e}}=\\mathbf{0}$.\n\n✏️ This follows from $\\left.  \\partial \\mathrm{SSE}(\\mathbf{b})/\\partial\\mathbf{b}\\right\\vert _{\\mathbf{b}=\\widehat{\\boldsymbol{\\beta}}\n}=\\mathbf{0}$, since $$-2\\mathbf{X}^{\\prime}(\\mathbf{y}-\\mathbf{X\\widehat{\\boldsymbol{\\beta}}})=-2\\mathbf{X}^{\\prime}\\widehat{\\mathbf{e}}=\\mathbf{0}$$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## OLS: showing the orthogonality of the residuals and predictors\nround(t(X)%*%resid(ols))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Analysis-of-variance*:\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}&=\\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2},\\\\\n\\text{Total Sum of Squares}&=\\text{Explained Sum of Squares} + \\text{Residual Sum of Squares},\\\\\n\\left(\\mathbf{y}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)^{\\prime}\\left(\\mathbf{y}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)&=\\left(\\widehat{\\mathbf{y}}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)^{\\prime}\\left(\\widehat{\\mathbf{y}}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)+\\widehat{\\mathbf{e}}^{\\prime} \\widehat{\\mathbf{e}}.\n\\end{aligned}\n$$\n\nwhere $\\boldsymbol{\\iota}_n$ is a $n\\times 1$ vector of ones."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "anova(ols)\n\n## OLS: total sum of squares\nsum(anova(ols)[,2])\n\n## OLS: explained sum of squares\nsum(anova(ols)[-9,2])\n\n## OLS: residual sum of squares\nsum(anova(ols)[9,2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Coefficient of Determination* (*$R^2$*):\n\n$$\nR^{2}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}.\n$$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## OLS: R2 (manually)\nsum(anova(ols)[-9,2])/sum(anova(ols)[,2])\n\n## OLS: R2 (from lm)\nsummary(ols)$r.squared",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Adjusted R-squared* ($\\overline{R}^2$):\n\n$$\n\\overline{R}^{2}=1-\\frac{(n-1) \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{(n-k) \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}\n$$\n\n✏️ Unlike the $R^2$ which cannot decrease as $k$ increases, $\\overline{R}^2$ can either increase _or_ decrease with $k$."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## OLS: adj. R2 (manually)\n1-(sum(anova(ols)[9,2])/sum(anova(ols)[,2]))*(sum(anova(ols)[,1])/anova(ols)[9,1])\n\n## OLS: adj. R2 (from lm)\nsummary(ols)$adj.r.squared",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Leverage Values*:\n\nThe [leverage values](https://en.wikipedia.org/wiki/Leverage_(statistics)) for the design matrix $\\mathbf{X}$ are the [diagonal](https://en.wikipedia.org/wiki/Main_diagonal) elements of the matrix $\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}$. There are $n$ leverage values, and are typically written as $h_{i i}$ for $i=1, \\ldots, n$. since\n\n$$\n\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}=\n\\left(\\begin{array}{c}{\\mathbf{x}_{1}^{\\prime}} \\\\ {\\mathbf{x}_{2}^{\\prime}} \\\\ {\\vdots} \\\\ {\\mathbf{x}_{n}^{\\prime}}\\end{array}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\begin{array}{llll}{\\mathbf{x}_{1}} & {\\mathbf{x}_{2}} & {\\cdots} & {\\mathbf{x}_{n}}\\end{array}\\right)\n$$\n\nthey are\n$$\nh_{i i}=\\mathbf{x}_{i}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{x}_{i}.\n$$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## OLS: leverage values\nhii <- hatvalues(ols)\nhead(hii)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Prediction Error* (leave-one-out residual or prediction residual):\n\n$$\\widetilde{e}_{i}=y_{i}-\\widetilde{y}_{i},$$ \n\nwhere we use the leave-one-out predicted value for $y_i$, i.e.,\n\n$$\n\\widetilde{y}_{i}=\\mathbf{x}_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{(-i)},$$\n\nand\n\n$$\n\\begin{aligned} \\widehat{\\boldsymbol{\\beta}}_{(-i)} &=\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\prime}\\right)^{-1}\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} y_{j}\\right) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}-\\mathbf{x}_{i} \\mathbf{x}_{i}^{\\prime}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{y}-\\mathbf{x}_{i} y_{i}\\right) \\\\ &=\\left(\\mathbf{X}_{(-i)}^{\\prime} \\mathbf{X}_{(-i)}\\right)^{-1} \\mathbf{X}_{(-i)}^{\\prime} \\mathbf{y}_{(-i)}. \\end{aligned}\n$$\n\nHere, $\\mathbf{X}_{(-i)}$ and $\\mathbf{y}_{(-i)}$ are the data matrices omitting the $i$th row. The notation $\\widehat{\\boldsymbol{\\beta}}_{(-i)}$ or $\\widehat{\\boldsymbol{\\beta}}_{-i}$ is commonly\nused to denote an estimator with the $i$th observation omitted.\n\n✏️ There is a leave-one-out estimator for each observation, $i=1,\\ldots,n$ so we have $n$ such estimators, and one can show that\n\n$$\n\\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\widehat{\\boldsymbol{\\beta}}-\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{x}_{i} \\widetilde{e}_{i},\n$$\n\nand\n\n$$\n\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}.\n$$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## OLS: original residuals\ne.hat <- resid(ols)\n\n## OLS: calculating the prediction error\ne.tilde <- resid(ols)/(1-hii)\n\n## OLS: manually re-calculating OLS without observation 156\ndata.frame(beta.hat=coef(ols),\n           beta.hat.i=coef(ols)-solve(t(X)%*%X)%*%X[156,]%*%e.tilde[156])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*Estimation of Error Variance*:\n\nThe _unconditional_ error variance $\\sigma^2=E(e^2_{i})$ can be estimated as\n\n1. Estimator 1:\n\n$$s^{2}=\\frac{1}{n-k} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}.$$\n\n2. Estimator 2:\n\n$$\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}.$$\n\n3. Estimator 3:\n\n$$\\bar{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\bar{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}^{2}.$$\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## OLS: error variance estimators\ns2 <- sum(e.hat^2)/(length(e.hat)-dim(X)[2])\nsigma.hat2 <- sum(e.hat^2)/length(e.hat)\nsigma.bar2 <- sum(e.hat^2/(1-hii))/length(e.hat)\ndata.frame(s2=s2,sigma.hat2=sigma.hat2,sigma.bar2=sigma.bar2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "✏️ When $k / n$ is small (typically, this occurs when $n$ is large), the estimators $\\widehat{\\sigma}^{2}, s^{2}$ and $\\overline{\\sigma}^{2}$ are likely to be similar to one another. However, if $k / n$ is large then $s^{2}$ and $\\overline{\\sigma}^{2}$ are generally preferred to $\\widehat{\\sigma}^{2}$. Consequently it is best to use one of the bias-corrected variance estimators in applications."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The _Finite Sample_ Properties of the OLS Estimator"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**<span style=\"color:blue\">Mean of OLS:</span>** Under Assumptions MLR.1, MLR.2, MLR.3, and MLR.4 one has\n\n$\\begin{aligned} E(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) &=E\\left(\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} | \\mathbf{X}\\right) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} E(\\mathbf{y} | \\mathbf{X}) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{X} \\boldsymbol{\\beta} \\\\ &=\\boldsymbol{\\beta} \\end{aligned}$\n\n**<span style=\"color:blue\">Variance of OLS:</span>** Firstly, let us use the notation $\\mathbf{V}_{\\widehat{\\beta}} \\stackrel{d e f}{=} \\text{var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X})$ and recall from Assumption MLR.5 that var$(\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X})=E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\\mathbf{D}$. Then under Assumptions MLR.1, MLR.2, MLR.3, MLR.4, and MLR.5 one has\n\n$\\begin{aligned} V_{\\widehat{\\beta}} &=\\text{var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{D} \\mathbf{X}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\end{aligned}$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "✏️ If one has _homoskedasticity_, i.e., $\\mathbf{D}=\\sigma^2\\mathbf{I}_n$, then $\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}}=\\sigma^{2}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}$."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Asymptotic Properties of the OLS Estimator"
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "r",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.5.3",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}