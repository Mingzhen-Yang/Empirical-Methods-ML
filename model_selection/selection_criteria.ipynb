{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Criteria: MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing everything from memory\n",
    "rm(list=ls())\n",
    "## turning all warnings off\n",
    "options(warn=-1)\n",
    "\n",
    "## for easy data manipulation and visualization\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "library(tidyverse)\n",
    "\n",
    "## provides helper functions for computing regression model performance metrics\n",
    "if (!require(modelr)) install.packages('modelr')\n",
    "library(modelr)\n",
    "\n",
    "## creates easily a tidy data frame containing the model statistical metrics\n",
    "if (!require(broom)) install.packages('broom')\n",
    "library(broom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first list selection criteria for the linear regression model $y_{i}=x_{i}^{\\prime} \\boldsymbol{\\beta}+e_{i}$ with $\\sigma^{2}=E\\left(e_{i}^{2}\\right)$ and a $(k+1)\\times 1$ coefficient vector $\\boldsymbol{\\beta}$. Let $\\widehat{\\boldsymbol{\\beta}}$ be the OLS estimator, $\\widehat{e}_{i}$ the OLS residual, and $\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}$ be the variance estimator. The number of estimated parameters ( $\\boldsymbol{\\beta}$ and $\\sigma^{2}$ ) is $K=k+2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "data(hprice3)\n",
    "\n",
    "## Obs:   321\n",
    "\n",
    "##  1. year                     1978, 1981\n",
    "##  2. age                      age of house\n",
    "##  3. agesq                    age^2\n",
    "##  4. nbh                      neighborhood, 1 to 6\n",
    "##  5. cbd                      dist. to central bus. dstrct, feet\n",
    "##  6. inst                     dist. to interstate, feet\n",
    "##  7. linst                    log(inst)\n",
    "##  8. price                    selling price\n",
    "##  9. rooms                    # rooms in house\n",
    "## 10. area                     square footage of house\n",
    "## 11. land                     square footage lot\n",
    "## 12. baths                    # bathrooms\n",
    "## 13. dist                     dist. from house to incin., feet\n",
    "## 14. ldist                    log(dist)\n",
    "## 15. lprice                   log(price)\n",
    "## 16. y81                      =1 if year = 1981\n",
    "## 17. larea                    log(area)\n",
    "## 18. lland                    log(land)\n",
    "## 19. linstsq                  linst^2\n",
    "\n",
    "hprice3.copy <- hprice3\n",
    "hprice3.copy$lcbd <- log(hprice3.copy$cbd)\n",
    "hprice3.copy$y81 <- as.factor(hprice3.copy$y81)\n",
    "\n",
    "## for easy data manipulation and visualization\n",
    "if (!require(caret)) install.packages('caret')\n",
    "library(caret)\n",
    "\n",
    "## creating dummy variable for y81==1 and dropping dummy y81==0\n",
    "dmy <- dummyVars(\" ~ y81\", data = hprice3.copy, fullRank=T)\n",
    "datos <- subset(hprice3.copy, select=c(\"lprice\",\"lland\",\"larea\",\"lcbd\",\"nbh\",\"rooms\",\n",
    "                                       \"linst\",\"linstsq\",\"ldist\",\"baths\",\"age\",\"agesq\"))\n",
    "datos <- cbind(datos,data.frame(predict(dmy, newdata = hprice3.copy)))\n",
    "head(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» The following code estimates two linear regression models ```model1``` uses all the predictors while ```model2``` excludes ```linstsq``` and ```agesq```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 <- lm(lprice ~., data = datos)\n",
    "model2 <- lm(lprice ~. -linstsq-agesq, data = datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_Adjusted $\\bar{R}^2$_**\n",
    "$$\n",
    "\\bar{R}^{2}=1-\\left(1-R^{2}\\right) \\frac{n-1}{n-K-1},\n",
    "$$\n",
    "where $R^2$ is the standard regression coefficient of determination.\n",
    "\n",
    "**_Bayesian Information Criterion_**\n",
    "$$\n",
    "\\mathrm{BIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+K \\log (n).\n",
    "$$\n",
    "**_Akaike Information Criterion_**\n",
    "$$\n",
    "\\mathrm{AIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+2 K.\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» Here, we use the function ```glance()``` to simply compare the overall quality of our two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics for model 1\n",
    "glance(model1) %>%\n",
    "  dplyr::select(adj.r.squared, BIC, AIC)\n",
    "\n",
    "## metrics for model 2\n",
    "glance(model2) %>%\n",
    "  dplyr::select(adj.r.squared, BIC, AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» The first model has higher $\\bar{R}^2$ and lower value for $\\mathrm{BIC}$ and $\\mathrm{AIC}$, so we would choose ```model1``` over ```model2```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_Mallows' $C_p$_**\n",
    "$$\n",
    "C_{p}=n \\widehat{\\sigma}^{2}+2 K \\widetilde{\\sigma}^{2},\n",
    "$$\n",
    "where $\\widetilde{\\sigma}^{2}$ is a preliminary estimator of $\\sigma^{2}$ (typically based on fitting a large model, i.e., the one containing all the predictors).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recovering sigma for models 1 and 2\n",
    "sigma1 <- glance(model1) %>%\n",
    "             dplyr::select(sigma)\n",
    "sigma2 <- glance(model2) %>%\n",
    "             dplyr::select(sigma)\n",
    "\n",
    "## calculating Mallows' Cp for models 1 and 2\n",
    "Cp1 <- nobs(model1)*(sigma1^2) + 2*(model1$rank)*(sigma1^2)\n",
    "Cp2 <- nobs(model2)*(sigma2^2) + 2*(model2$rank)*(sigma1^2)\n",
    "\n",
    "cbind(Cp1,Cp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» Again, the ```model1``` has smaller $C_p$ than ```model2```, so ```model1``` is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_Shibata_**\n",
    "$$\n",
    "\\text{Shibata}=\\widehat{\\sigma}^{2}\\left(1+\\frac{2 K}{n}\\right).\n",
    "$$\n",
    "\n",
    "**_Final Prediction Error_**\n",
    "$$\n",
    "\\mathrm{FPE}=\\widehat{\\sigma}^{2}\\left(\\frac{1+K / n}{1-K / n}\\right).\n",
    "$$\n",
    "\n",
    "**_Generalized Cross-Validation_**\n",
    "$$\n",
    "\\mathrm{GCV}=\\frac{n \\widehat{\\sigma}^{2}}{(n-K)^{2}}.\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## manually calculating Shibata, FPE, and GCV for model1\n",
    "shibata1 <- (sigma1^2)*(1+2*model1$rank/nobs(model1))\n",
    "FPE1 <- (sigma1^2)*(1+model1$rank/nobs(model1))/(1-model1$rank/nobs(model1))\n",
    "GCV1 <- nobs(model1)*(sigma1^2)/(nobs(model1)-model1$rank)\n",
    "\n",
    "## manually calculating Shibata, FPE, and GCV for model2\n",
    "shibata2 <- (sigma2^2)*(1+2*model2$rank/nobs(model2))\n",
    "FPE2 <- (sigma1^2)*(1+model2$rank/nobs(model2))/(1-model2$rank/nobs(model2))\n",
    "GCV2 <- nobs(model2)*(sigma2^2)/(nobs(model2)-model2$rank)\n",
    "\n",
    "data.frame(shibata=unname(rbind(shibata1,shibata2)),\n",
    "           FPE=unname(rbind(FPE1,FPE2)),\n",
    "           GCV=unname(rbind(GCV1,GCV2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» Again, the ```model1``` has smaller $\\mathrm{Shibata}$ and $\\mathrm{GCV}$ than ```model2```, but ```model2``` is preferred based on the $\\mathrm{FPE}$ criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_Cross-Validation_**\n",
    "$$\n",
    "\\mathrm{CV}=\\frac{1}{n}\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2},\n",
    "$$\n",
    "where $\\widetilde{e}_{i}$ are the least squares leave-one-out prediction errors.\n",
    "\n",
    "<ins>Prediction erros</ins>: We define the leave-one-out estimator as that obtained by applying an estimation formula to the sample omitting the $i$th observation, i.e.,\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\widehat{\\boldsymbol{\\beta}}-\\frac{1}{\\left(1-h_{i i}\\right)}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{x}_{i} \\widehat{e}_{i},\n",
    "$$\n",
    "\n",
    "where $\\widehat{e}_{i}$ are the least squares residuals and $h_{ii}$ are the [leverage](https://en.wikipedia.org/wiki/Leverage_(statistics)) values. We also define the leave-one-out residual or prediction error as that obtained using the leave-one-out regression estimator, thus\n",
    "\n",
    "$$\n",
    "\\tilde{e}_{i}=y_{i}-x_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}.\n",
    "$$\n",
    "\n",
    "We define the out-of-sample mean squared error as\n",
    "$$\n",
    "\\tilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2}\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating manually the CV criteria for models 1 and 2\n",
    "CV1 <- mean((resid(model1)/(1 - hatvalues(model1)))^2) \n",
    "CV2 <- mean((resid(model2)/(1 - hatvalues(model2)))^2)\n",
    "data.frame(CV=unname(rbind(CV1,CV2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» Again, the ```model1``` has smaller $\\mathrm{CV}$ than ```model2```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
