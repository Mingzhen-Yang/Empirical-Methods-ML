{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Suppose that we have a $K\\times 1$ estimator $\\hat{\\boldsymbol{\\theta}}$ which has mean $\\boldsymbol{\\theta}$ and variance-covariance matrix $\\boldsymbol{V}$. An alternative feasible estimator is $\\tilde{\\boldsymbol{\\theta}}=\\boldsymbol{0}$. The latter may seem like a silly estimator, but it captures the feature that model selection typically takes the form of exclusion restrictions setting coefficients to 0. In this context we can compare the accuracy of the two estimators by their weighted mean-squared error (WMSE). For a given weight matrix $\\boldsymbol{W}$ define\n",
    "\n",
    "$$\n",
    "\\text{WMSE}(\\hat{\\boldsymbol{\\theta}})=\\text{tr}\\left(\\mathrm{E}\\left((\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})(\\boldsymbol{\\theta}-\\boldsymbol{\\theta})^{\\prime}\\right) \\boldsymbol{W}\\right)=\\mathrm{E}\\left((\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})^{\\prime} \\boldsymbol{W}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})\\right).\n",
    "$$\n",
    "\n",
    "The calculations simplify by setting $\\boldsymbol{W}=\\boldsymbol{V}^{-1},$ which we do for our remaining calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our two estimators we calculate that\n",
    "$$\n",
    "\\begin{array}{l}{\\text { WMSE }(\\widehat{\\boldsymbol{\\theta}})=K} \\\\ {\\text {WMSE }(\\tilde{\\boldsymbol{\\theta}})=\\boldsymbol{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\boldsymbol{\\theta} \\stackrel{\\text {def}}{=} \\lambda.}\\end{array}\n",
    "$$\n",
    "\n",
    "The WMSE of $\\widehat{\\boldsymbol{\\theta}}$ is smaller if $K>\\lambda$ and the WMSE of $\\tilde{\\boldsymbol{\\theta}}$ is smaller if $K<\\lambda$. One insight from this simple analysis is that we should prefer smaller (simpler) models when potentially omitted variables have small coefficients relative to estimation variance, and should prefer larger (more complicated) models when\n",
    "these variables have large coefficients relative to estimation variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a somewhat broader comparison. Suppose $\\hat{\\boldsymbol{\\theta}}$ is $\\overline{K} \\times 1$ with mean $\\boldsymbol{\\theta}$ and variance matrix $V$. For some $\\bar{K} \\times(\\bar{K}-K)$ full-rank matrix $\\boldsymbol{R}$ consider\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\text { WMSE }(\\tilde{\\boldsymbol{\\theta}}) &=E\\left((\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})^{\\prime} \\boldsymbol{V}^{-1}(\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})\\right) \\\\ &=\\boldsymbol{\\theta}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{\\theta}+K \\end{aligned}\n",
    "$$\n",
    "\n",
    "The first term is the squared bias, the second is the weighted variance. This simple expression illustrates the basic bias-variance trade-off. Increasing $K$ increases the estimation variance but decreases the squared bias, the latter by decreasing the rank of $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias can be estimated by replacing $\\hat{\\boldsymbol{\\theta}}$ with $\\boldsymbol{\\theta}$. This squared bias estimate is biased since\n",
    "\n",
    "$$\n",
    "E\\left[\\widehat{\\boldsymbol{\\theta}}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\theta}}\\right]=\\boldsymbol{\\theta}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{\\theta}+\\bar{K}-K.\n",
    "$$\n",
    "\n",
    "Putting these calculations together we see that an unbiased estimator for the weighted MSE is\n",
    "\n",
    "$$\n",
    "\\begin{aligned} M_{K} &=\\widehat{\\boldsymbol{\\theta}}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\theta}}+2 K-\\bar{K} \\\\ &=(\\widehat{\\boldsymbol{\\theta}}-\\widetilde{\\boldsymbol{\\theta}})^{\\prime} V^{-1}(\\widehat{\\boldsymbol{\\theta}}-\\widetilde{\\boldsymbol{\\theta}})+2 K-\\bar{K} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Theorem:** If $\\widehat{\\boldsymbol{\\theta}}$ has mean $\\boldsymbol{\\theta}$ and variance $\\boldsymbol{V}$ and $\\tilde{\\boldsymbol{\\theta}}=\\widehat{\\boldsymbol{\\theta}}-\\boldsymbol{V} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\hat{\\boldsymbol{\\theta}}$\n",
    "then $E\\left(M_{K}\\right)=\\text { WMSE }(\\tilde{\\boldsymbol{\\theta}})-\\text { WMSE }(\\widehat{\\boldsymbol{\\theta}})$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor $\\bar{K}$ in $M_{K}$ is constant across models so can be omitted for the purposes of model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice $V$ is unknown. It can be replaced with a consistent estimator and we arrive at the <font color='red'>MSE Selection Criterion</font>\n",
    "$$\n",
    "\\begin{aligned} M_{K} &=\\widehat{\\boldsymbol{\\theta}}^{\\prime} R\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\hat{\\boldsymbol{\\theta}}+2 K \\\\ &=(\\widehat{\\boldsymbol{\\theta}}-\\tilde{\\boldsymbol{\\theta}})^{\\prime} \\widehat{\\boldsymbol{V}}^{-1}(\\widehat{\\boldsymbol{\\theta}}-\\tilde{\\boldsymbol{\\theta}})+2 K. \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE selection picks the model for which the estimated WMSE $M_{K}$ is the smallest. For implementation, a set of models are estimated, $M_{K}$ calculated, and the model with the smallest $M_{K}$ selected.\n",
    "\n",
    "<ins>Note</ins>: The MSE selection criterion described here is not a common model selection tool, but we have presented it as it is the simplest to derive and understand. Furthermore, it turns out to be quite similar to several popular methods, as we show later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Criteria: MLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first list selection criteria for the linear regression model $y_{i}=x_{i}^{\\prime} \\boldsymbol{\\beta}+e_{i}$ with $\\sigma^{2}=E\\left(e_{i}^{2}\\right)$ and a $(k+1)\\times 1$ coefficient vector $\\boldsymbol{\\beta}$. Let $\\widehat{\\boldsymbol{\\beta}}$ be the OLS estimator, $\\widehat{e}_{i}$ the OLS residual, and $\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}$ be the variance estimator. The number of estimated parameters ( $\\boldsymbol{\\beta}$ and $\\sigma^{2}$ ) is $K=k+2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Adjusted $\\bar{R}^2$_**\n",
    "$$\n",
    "\\bar{R}^{2}=1-\\left(1-R^{2}\\right) \\frac{n-1}{n-K-1},\n",
    "$$\n",
    "where $R^2$ is the standard regression coefficient of determination.\n",
    "\n",
    "**_Bayesian Information Criterion_**\n",
    "$$\n",
    "\\mathrm{BIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+K \\log (n).\n",
    "$$\n",
    "**_Akaike Information Criterion_**\n",
    "$$\n",
    "\\mathrm{AIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+2 K.\n",
    "$$\n",
    "\n",
    "**_Mallows' $C_p$_**\n",
    "$$\n",
    "C_{p}=n \\widehat{\\sigma}^{2}+2 K \\widetilde{\\sigma}^{2},\n",
    "$$\n",
    "where $\\widetilde{\\sigma}^{2}$ is a preliminary estimator of $\\sigma^{2}$ (typically based on fitting a large model, i.e., the one containing all the predictors).\n",
    "\n",
    "**_Shibata_**\n",
    "$$\n",
    "\\text{Shibata}=\\widehat{\\sigma}^{2}\\left(1+\\frac{2 K}{n}\\right).\n",
    "$$\n",
    "\n",
    "**_Final Precition Error_**\n",
    "$$\n",
    "\\mathrm{FPE}=\\widehat{\\sigma}^{2}\\left(\\frac{1+K / n}{1-K / n}\\right).\n",
    "$$\n",
    "\n",
    "**_Cross-Validation_**\n",
    "$$\n",
    "\\mathrm{CV}=\\frac{1}{n}\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2},\n",
    "$$\n",
    "where $\\widetilde{e}_{i}$ are the least squares leave-one-out prediction errors.\n",
    "\n",
    "<ins>Prediction erros</ins>: We define the leave-one-out estimator as that obtained by applying an estimation formula to the sample omitting the $i$th observation, i.e.,\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\widehat{\\boldsymbol{\\beta}}-\\frac{1}{\\left(1-h_{i i}\\right)}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{x}_{i} \\widehat{e}_{i},\n",
    "$$\n",
    "\n",
    "where $\\widehat{e}_{i}$ are the least squares residuals and $h_{ii}$ are the [leverage](https://en.wikipedia.org/wiki/Leverage_(statistics)) values. We also define the leave-one-out residual or prediction error as that obtained using the leave-one-out regression estimator, thus\n",
    "\n",
    "$$\n",
    "\\tilde{e}_{i}=y_{i}-x_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}.\n",
    "$$\n",
    "\n",
    "We define the out-of-sample mean squared error as\n",
    "$$\n",
    "\\tilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2}\n",
    "$$\n",
    "\n",
    "**_Generalized Cross-Validation_**\n",
    "$$\n",
    "\\mathrm{GCV}=\\frac{n \\widehat{\\sigma}^{2}}{(n-K)^{2}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
