\documentclass{article}%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}%
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2953}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{Created=Saturday, November 09, 2019 14:35:30}
%TCIDATA{LastRevised=Sunday, November 10, 2019 09:39:10}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\begin{document}
$%
\begin{array}
[c]{c}%
y_{1}=\beta_{0}+\beta_{1}x_{1,1}+\cdots+\beta_{k}x_{1,k}+u_{1}\\
y_{2}=\beta_{0}+\beta_{1}x_{2,1}+\cdots+\beta_{k}x_{2,k}+u_{2}\\
y_{3}=\beta_{0}+\beta_{1}x_{3,1}+\cdots+\beta_{k}x_{3,k}+u_{3}\\
\vdots\\
y_{n}=\beta_{0}+\beta_{1}x_{n,1}+\cdots+\beta_{k}x_{n,k}+u_{n}%
\end{array}
$ or equivalently $\left[
\begin{array}
[c]{c}%
y_{1}\\
y_{2}\\
y_{3}\\
\vdots\\
y_{n}%
\end{array}
\right]  =\left[  {}\right]  \left[
\begin{array}
[c]{c}%
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}%
\end{array}
\right]  +\left[
\begin{array}
[c]{c}%
u_{1}\\
u_{2}\\
u_{3}\\
\vdots\\
u_{n}%
\end{array}
\right]  $

\bigskip

$E[\mathbf{u}|\mathbf{X}]=\left[
\begin{array}
[c]{c}%
E[u_{1}|\mathbf{X}]\\
E[u_{2}|\mathbf{X}]\\
E[u_{3}|\mathbf{X}]\\
\vdots\\
E[u_{n}|\mathbf{X}]
\end{array}
\right]  =\left[
\begin{array}
[c]{c}%
0\\
0\\
0\\
\vdots\\
0
\end{array}
\right]  $%

\[
E[\mathbf{u}\mathbf{u}^{\prime}|\mathbf{X}]=\left[
\begin{array}
[c]{cccc}%
E[u_{1}^{2}|\mathbf{X}] & E[u_{1}u_{2}|\mathbf{X}] & \cdots & E[u_{1}%
u_{n}|\mathbf{X}]\\
E[u_{2}u_{1}|\mathbf{X}] & E[u_{2}^{2}|\mathbf{X}] & \cdots & E[u_{2}%
u_{n}|\mathbf{X}]\\
E[u_{3}u_{1}|\mathbf{X}] & E[u_{3}u_{2}|\mathbf{X}] & \cdots & E[u_{3}%
u_{n}|\mathbf{X}]\\
\vdots & \vdots & \ddots & \vdots\\
E[u_{n}u_{1}|\mathbf{X}] & E[u_{n}u_{2}|\mathbf{X}] & \cdots & E[u_{n}%
^{2}|\mathbf{X}]
\end{array}
\right]  =\Omega
\]


\bigskip

Ordinary Least Squares (OLS) Estimator

\bigskip

Define the residual sum of squares (RSS) as a function of any candiate guess,
$\mathbf{b}=[b_{0},b_{1},\cdots,b_{k}]^{\prime}$, for the unknown $[\beta
_{0},\beta_{1},\beta_{2},\cdots,\beta_{k}]^{\prime}\equiv\mathbf{\beta}$,
i.e.,%
\[
RSS(\mathbf{b})=\Sigma_{i=1}^{n}(y_{1}-b_{0}-b_{1}x_{1}-b_{2}x_{2}%
-\cdots-b_{k}x_{k})^{2}=(\mathbf{y}-\mathbf{Xb})^{\prime}(\mathbf{y}%
-\mathbf{Xb})
\]
By standard matrix calculus one has%
\[
\frac{\partial RSS(\mathbf{b})}{\partial\mathbf{b}}=\left[
\begin{array}
[c]{c}%
\partial RSS(\mathbf{b})/\partial b_{0}\\
\partial RSS(\mathbf{b})/\partial b_{1}\\
\partial RSS(\mathbf{b})/\partial b_{2}\\
\vdots\\
\partial RSS(\mathbf{b})/\partial b_{k}%
\end{array}
\right]  =-2\mathbf{X}^{\prime}(\mathbf{y}-\mathbf{Xb})\text{, and }%
\frac{\partial^{2}RSS(\mathbf{b})}{\partial\mathbf{b}\partial\mathbf{b}%
^{\prime}}=\left[
\begin{array}
[c]{cccc}%
\partial^{2}RSS(\mathbf{b})/\partial b_{0}^{2} & \partial^{2}RSS(\mathbf{b}%
)/\partial b_{0}\partial b_{1} & \cdots & \partial^{2}RSS(\mathbf{b})/\partial
b_{0}\partial b_{k}\\
\partial^{2}RSS(\mathbf{b})/\partial b_{1}\partial b_{0} & \partial
^{2}RSS(\mathbf{b})/\partial b_{1}^{2} & \cdots & \partial^{2}RSS(\mathbf{b}%
)/\partial b_{1}\partial b_{k}\\
\partial^{2}RSS(\mathbf{b})/\partial b_{2}\partial b_{0} & \partial
^{2}RSS(\mathbf{b})/\partial b_{2}\partial b_{1} & \cdots & \partial
^{2}RSS(\mathbf{b})/\partial b_{2}\partial b_{k}\\
\vdots & \vdots & \ddots & \vdots\\
\partial^{2}RSS(\mathbf{b})/\partial b_{k}\partial b_{0} & \partial
^{2}RSS(\mathbf{b})/\partial b_{k}\partial b_{1} & \cdots & \partial
^{2}RSS(\mathbf{b})/\partial b_{k}^{2}%
\end{array}
\right]  =2\mathbf{X}^{\prime}\mathbf{X}\text{,}%
\]
where $\mathbf{X}^{\prime}\mathbf{X}$ is a positive definite matrix by
Assumption MLR.3.


\end{document}