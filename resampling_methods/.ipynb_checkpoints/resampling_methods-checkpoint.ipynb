{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two most commonly used _resampling methods,_ _cross-validation,_ and the _bootstrap_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing everything from memory\n",
    "rm(list=ls())\n",
    "## turning all warnings off\n",
    "options(warn=-1)\n",
    "\n",
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "## loading the packages\n",
    "library(wooldridge)\n",
    "\n",
    "## see the raw data\n",
    "head(hprice3)\n",
    "\n",
    "## pre-processing the data set\n",
    "hprice3_processed <- subset(hprice3,select=c(\"lprice\",\"lland\",\"larea\",\"nbh\",\"rooms\",\"cbd\",\"y81\",\"ldist\",\"baths\",\"age\",\"agesq\"))\n",
    "hprice3_processed$cbd <- log(hprice3_processed$cbd)\n",
    "hprice3_processed$y81 <- as.factor(hprice3_processed$y81)\n",
    "hprice3_processed$nbh <- as.factor(hprice3_processed$nbh)\n",
    "\n",
    "head(hprice3_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are used to do two things\n",
    "\n",
    "1. _Model Assessment_ - the process of evaluating a model's performance.\n",
    "2. _Model Selection_ - the process of selecting the proper level of flexibility for a model.\n",
    "\n",
    "Since models are 'trained' using training data sets, they are by construction suitable to fit data in these training data sets only (they were fit by minimizing in-sample fitted errors). Since the validation set was not used to fit the model, these set of observations can be used to assess the performance of the model and therefore will allow us to do model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## installing the 'caret' package if not previously installed\n",
    "if (!require(caret)) install.packages('caret')\n",
    "\n",
    "## loading the packages\n",
    "library(caret)\n",
    "\n",
    "## continue pre-processing\n",
    "## converting every categorical variable to numerical using dummy variables\n",
    "dmy <- dummyVars(\" ~ .\", data = hprice3_processed,fullRank = T)\n",
    "hprice3_processed <- data.frame(predict(dmy, newdata = hprice3_processed))\n",
    "\n",
    "## looking at the structure of caret package.\n",
    "str(hprice3_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![validation_set_approach.png](img/validation_set_approach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schematic display of the validation set approach. A set of $n$ observations are randomly split into a training set (shown in blue, containing observations 7, 22, and 13, among others) and a validation set (shown in beige, and containing observation 91, among others). The statistical learning method is fit on the training set, and its performance is evaluated on the validation set.\n",
    "\n",
    "This can be done many (say $J$) times randomly and then all model assessment measures like $RMSE$, $\\bar{R}^2$, $C_p$, $BIC$, and $AIC$ can be calculated many (say $J$) times.\n",
    "\n",
    "<ins>*Note*</ins>: Recall that if we have a sample $\\{y_1,y_2,\\ldots,y_m\\}$ for which we have predicted $\\{\\widehat{y}_1,\\widehat{y}_2,\\ldots,\\widehat{y}_m\\}$, then the $MSE$ for these samples is defined as\n",
    "\n",
    "$$\n",
    " \\begin{align*}\n",
    " MSE = \\frac{1}{m}\\sum_{j=1}^m (y_j-\\widehat{y}_j)^2\\text{,}\n",
    " \\end{align*}\n",
    "$$ \n",
    " and the $RMSE$ is simply defined as $RMSE=\\sqrt{MSE}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for reproducibility\n",
    "set.seed(42)\n",
    "\n",
    "## spliting processed data set into two parts based on outcome: 80% and 20%\n",
    "index <- createDataPartition(hprice3_processed$lprice, p=0.80, list=FALSE)\n",
    "trainSet <- hprice3_processed[ index,]\n",
    "validationSet <- hprice3_processed[-index,]\n",
    "\n",
    "## checking the structure of trainSet\n",
    "str(trainSet)\n",
    "\n",
    "## checking the structure of validationSet\n",
    "str(validationSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__Example__**: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train a linear regression model\n",
    "model <- lm(lprice~., data=trainSet)\n",
    "summary(model)\n",
    "\n",
    "## making predictions\n",
    "predictions <- predict(model, validationSet)\n",
    "\n",
    "## summarize results\n",
    "postResample(pred = predictions, obs = validationSet$lprice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LOOCV.png](img/LOOCV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schematic display of **LOOCV**. A set of $n$ data points is repeatedly split into a training set (shown in blue) containing all but one observation, and a validation set that contains only that observation (shown in beige). The test error is then estimated by averaging the $n$ resulting $MSE$’s. The first training set contains all but observation 1, the second training set contains all but observation 2, and so forth.\n",
    "\n",
    "Notice that a total of $n$ training data sets containing exactly $n-1$ observations have been constructed along with $n$ corresponding validation data sets containing exactly $1$ observation each.\n",
    "\n",
    "<ins>*Note*</ins>: Recall that in this case we will be using the training data set $\\{(y_1,\\mathbf{x}_1^\\prime),\\ldots,(y_{i-1},\\mathbf{x}_{i-1}^\\prime),(y_{i+1},\\mathbf{x}_{i+1}^\\prime),\\ldots,(y_n,\\mathbf{x}_n^\\prime)\\}$ to predict $y_i$ for $i=1,\\ldots,n$. Therefore the $MSE$ is defined as before for pairs $\\{(y_1,\\widehat{y}_1),\\ldots,(y_n,\\widehat{y}_n)\\}$, i.e., $MSE_i=(y_1-\\widehat{y}_1)^2$, $MSE=n^{-1}\\sum_{i=1}^{n}MSE_i$, and $RMSE=\\sqrt{MSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__Example__**: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define training control\n",
    "train_control <- trainControl(method=\"LOOCV\")\n",
    "\n",
    "## train the model\n",
    "model <- train(lprice~., data=hprice3_processed, trControl=train_control, method=\"glm\")\n",
    "\n",
    "## summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![k_fold_CV.PNG](img/k_fold_CV.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schematic display of 5-fold CV. A set of $n$ observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting $MSE$ estimates.\n",
    "\n",
    "Notice that this approach involves randomly dividing the set of observations into $k$ groups (called __folds__), of approximately equal size. The first fold is treated as a validaton set, and the method is fit on the remaining $k−1$ folds.\n",
    "\n",
    "<ins>*Note*</ins>: For each fold $k$, notice that we can calculate the corresponding $MSE$ using the validation set for that fold. Therefore, we will have $MSE_1,\\ldots,MSE_k$ so $MSE=k^{-1}\\sum_{j=1}^{k}MSE_j$, and $RMSE=\\sqrt{MSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__Example__**: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define training control\n",
    "train_control <- trainControl(method=\"cv\", number=10)\n",
    "\n",
    "## set the seed for reproducibility\n",
    "set.seed(42)\n",
    "\n",
    "## train the model\n",
    "model <- train(lprice~., data=hprice3_processed, trControl=train_control, method=\"glm\")\n",
    "\n",
    "## summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If $k=n$ the method is identical to _leave-one-out cross validation_.\n",
    "\n",
    "⚠️ A disadvantage of $k$-fold cross-validation is that the results can be sensitive to the initial random sorting of the observations. Consequently some practitioners calculate the criterion $M$ times and then average the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
